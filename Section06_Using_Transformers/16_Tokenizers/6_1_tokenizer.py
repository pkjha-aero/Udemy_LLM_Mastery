# -*- coding: utf-8 -*-
"""6.1 Tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wg_2Q9UkVDOhL7g9bdB6CvE_ww1UZ8WM

# ü§ñ **Understanding Tokenizers with BERT**

This notebook shows how to use BERT tokenizers to turn text into data that the model can understand.

## üõ†Ô∏è Setup and Installation

First, we need to install the libraries we will use.
"""

!pip install pandas==2.0.1
!pip install transformers==4.29.2

"""
## üìö Importing Libraries

We import the libraries necessary for our tasks."""

# Import required libraries
from transformers import BertModel, AutoTokenizer
import pandas as pd

"""
## ü§ñ Model Setup

We load a pre-trained BERT model and its tokenizer."""

# Specify the pre-trained model to use: BERT-base-cased
model_name = "bert-base-cased"

# Instantiate the model and tokenizer for the specified pre-trained model
model = BertModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer

"""
## üìù Tokenizing Text

We use the tokenizer to turn a sentence into tokens."""

# Set a sentence for analysis
sentence = "When life gives you lemons, don't make lemonade."

# Tokenize the sentence
tokens = tokenizer.tokenize(sentence)
tokens

"""
## üìò Vocabulary and Token IDs

We create a DataFrame to see the tokenizer's vocabulary and sort it by token IDs."""

# Create a DataFrame with the tokenizer's vocabulary
vocab = tokenizer.vocab
vocab_df = pd.DataFrame({"token": vocab.keys(), "token_id": vocab.values()})
vocab_df = vocab_df.sort_values(by="token_id").set_index("token_id")

vocab_df

"""## üîç Encoding and Decoding

Encode the sentence into IDs and then decode it back to text.
"""

# Encode the sentence into token_ids using the tokenizer
token_ids = tokenizer.encode(sentence)
token_ids

"""
## üîé Compare Token Lengths

Compare the length of tokens and token IDs.
"""

# Print the length of tokens and token_ids
print("Number of tokens:", len(tokens))
print("Number of token IDs:", len(token_ids))

"""
## üîÑ Explore Token Data

Look at specific tokens by their IDs."""

# Access the tokens in the vocabulary DataFrame by index
print("Token at position 101:", vocab_df.iloc[101])
print("Token at position 102:", vocab_df.iloc[102])

"""## üìÉ Token and ID Pairing

Show pairs of tokens and their IDs.
"""

# Zip tokens and token_ids (excluding the first and last token_ids for [CLS] and [SEP])
list(zip(tokens, token_ids[1:-1]))

# Decode the token_ids (excluding the first and last token_ids for [CLS] and [SEP]) back into the original sentence
tokenizer.decode(token_ids[1:-1])

# Tokenize the sentence using the tokenizer's `__call__` method
tokenizer_out = tokenizer(sentence)
tokenizer_out

"""
## üß© Handling Multiple Sentences

Tokenize two sentences with and without padding, and decode them."""

# Create a new sentence by removing "don't " from the original sentence
sentence2 = sentence.replace("don't ", "")
sentence2

# Tokenize both sentences with padding
tokenizer_out2 = tokenizer([sentence, sentence2], padding=True)
tokenizer_out2

# Decode the tokenized input_ids for both sentences
tokenizer.decode(tokenizer_out2["input_ids"][0])

tokenizer.decode(tokenizer_out2["input_ids"][1])

"""
## üåü Conclusion

This notebook walked you through how to use a BERT tokenizer to process text, turning it into tokens and IDs, and how to handle multiple sentences. Feel free to change the sentences or explore more functions of the tokenizer."""